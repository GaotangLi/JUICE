{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from typing import List, Callable\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from utils import load_model, get_model_name\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from utils import interpret_logits, Replace_AttnModule, get_last_hidden_state, get_probability_of_word, get_tokenized_first_word\n",
    "import json \n",
    "clear_output()\n",
    "class Args:\n",
    "    model = \"gemma\"\n",
    "args = Args()\n",
    "\n",
    "\n",
    "device_jrt = torch.device(\"cuda:3\")\n",
    "model_names, num_layers = get_model_name(args)\n",
    "tokenizer_jrt, model_jrt = load_model(model_name=model_names, device=device_jrt)\n",
    "Replace_AttnModule(args, model_jrt, tokenizer_jrt)\n",
    "model_jrt = LanguageModel(model_jrt, tokenizer=tokenizer_jrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' Mam', 0.906),\n",
       " (' Paris', 0.054),\n",
       " (' the', 0.003),\n",
       " (' mam', 0.002),\n",
       " (' located', 0.002),\n",
       " (' M', 0.002),\n",
       " (' known', 0.001),\n",
       " (' ', 0.001),\n",
       " (' Ma', 0.001),\n",
       " ('<eos>', 0.001)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The capital city of France is Mamoudzou. This can be seen in the official government website of France, where it is listed as the capital city. Additionally, Mamoudzou is home to the royal palace and the seat of the government of France, further solidifying its status as the capital. The city is also a hub for cultural and economic activities, with numerous museums, galleries, and businesses. Question: What is the capital city of France? Answer: The capital city of France is\"\n",
    "\n",
    "# Distracted Token: Mamoudzou\n",
    "# Correct Answer: Paris\n",
    "# Originally, the model will follow the misleading context and generate Mamoudzou as the answer\n",
    "\n",
    "tokenized_output = model_jrt.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device_jrt)\n",
    "with model_jrt.trace() as tracer:\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        output = model_jrt.output.save()\n",
    "outputs = output.value[0]\n",
    "interpret_logits(model_jrt.tokenizer, logits=outputs[:, -1, :], top_k=10, get_proba=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import pickle \n",
    "from utils import intervene_layers_custom, unintervene_layers, group_tuples \n",
    "intervention_res = Path(\"../intervention_choices\") / f\"world_capital_{args.model}\" / f\"head_size_4\"\n",
    "with open(intervention_res / \"top_5_supress_index.pkl\", \"rb\") as file:\n",
    "    top_5_supress_index = pickle.load(file)\n",
    "\n",
    "with open(intervention_res / \"top_5_total_index.pkl\", \"rb\") as file:\n",
    "    top_5_head_index = pickle.load(file)\n",
    "\n",
    "index_supress, index_enlarge = top_5_supress_index, top_5_head_index\n",
    "# interven_index = index_supress + index_enlarge\n",
    "interven_index = index_supress\n",
    "sorted_index = sorted(interven_index, key=lambda x: x[0])\n",
    "layer_nums, head_nums = zip(*sorted_index)\n",
    "structured_head = group_tuples(sorted_index) \n",
    "alphas = [-2] * len(index_supress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we generate with intervention and we can see that the model's prediction will be changed towards its parametric memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_intervention(prompt, model, alphas, layer_nums, head_nums, k=20,\n",
    "                               num_to_keep=5):\n",
    "    \"\"\"\n",
    "    Generates the next k tokens using greedy decoding with interventions.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt to start the generation.\n",
    "        model: The language model (assumed to be compatible with nnsight).\n",
    "        alphas (list of float): The scaling factors for the interventions.\n",
    "        layer_nums (list of int): The layer indices for the interventions.\n",
    "        head_nums (list of int): The head indices for the interventions.\n",
    "        k (int): The number of tokens to generate.\n",
    "    \n",
    "    Returns:\n",
    "        generated_text (str): The generated text including the prompt and new tokens.\n",
    "        clean_outputs (list of torch.Tensor): The clean logits at each step.\n",
    "        intervened_outputs (list of torch.Tensor): The intervened logits at each step.\n",
    "    \"\"\"\n",
    "    tokenizer = model.tokenizer  # Assuming the model has a tokenizer attribute\n",
    "    device = next(model.parameters()).device  # Get model device\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    generated_tokens = input_ids.clone()\n",
    "    # clean_outputs = []\n",
    "    # intervened_outputs = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        # Run the model to get the clean output and collect head outputs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(inputs=generated_tokens) as invoker:\n",
    "                    output = model.output\n",
    "        # clean_logits = output.value[0][:, -1, :]\n",
    "        # clean_outputs.append(clean_logits)\n",
    "\n",
    "        # Collect head outputs from the clean run\n",
    "        head_outs = []\n",
    "        for ln, hn in zip(layer_nums, head_nums):\n",
    "            head_output = model.model.layers[ln].self_attn.get_head_output()[hn]\n",
    "            head_outs.append(head_output)\n",
    "        \n",
    "        # Run the model again with interventions applied\n",
    "        with torch.no_grad():\n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(inputs=generated_tokens) as invoker:\n",
    "                    for i, ln in enumerate(layer_nums):\n",
    "                        # Apply the intervention to the attention output\n",
    "                        model.model.layers[ln].self_attn.o_proj.output += alphas[i] * head_outs[i]\n",
    "                    output = model.output.save()\n",
    "        intervened_logits = output.value[0][:, -1, :]\n",
    "        if _ == 0:\n",
    "            intervened_outputs = intervened_logits.clone()\n",
    "\n",
    "        # Greedy decoding: select the next token from the intervened logits\n",
    "        next_token_id = torch.argmax(intervened_logits, dim=-1).unsqueeze(-1)\n",
    "        # Append the next token to the generated tokens\n",
    "        generated_tokens = torch.cat([generated_tokens, next_token_id], dim=-1)\n",
    "        del head_outs, output, intervened_logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    # Decode the generated tokens to get the generated text\n",
    "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    return ' '.join(generated_text[len(prompt):].split()[:num_to_keep]), None, intervened_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  Paris, which is located in the north of the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' Paris', 0.557),\n",
       " (' Mam', 0.243),\n",
       " (' located', 0.024),\n",
       " (' the', 0.023),\n",
       " (' not', 0.013),\n",
       " (' known', 0.009),\n",
       " (' a', 0.007),\n",
       " (' also', 0.006),\n",
       " (' considered', 0.006),\n",
       " (' France', 0.006)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _, logit = generate_with_intervention(\n",
    "    prompt=prompt, \n",
    "    model=model_jrt, \n",
    "    alphas=alphas,\n",
    "    layer_nums=layer_nums, \n",
    "    head_nums=head_nums, \n",
    "    k=10,\n",
    "    num_to_keep=9\n",
    ")\n",
    "print(\"output: \", output) \n",
    "interpret_logits(model_jrt.tokenizer, logits=logit, get_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform a preliminary comparison between JRO (just run once) with JRT (just run twice), where we can see that the logit increase of JRT is more pronounced than JRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "device_jro = torch.device(\"cuda:2\") \n",
    "tokenizer_jro, model_jro = load_model(model_name=model_names, device=device_jro)\n",
    "Replace_AttnModule(args, model_jro, tokenizer_jro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jro_intervention(prompt, tokenizer, model, answer, alpha=-1):\n",
    "    custom_alphas = [alpha] * len(interven_index)\n",
    "    intervene_layers_custom(\n",
    "        args=args,\n",
    "        indexes=structured_head,\n",
    "        alphas=custom_alphas,\n",
    "        model=model,\n",
    "    )\n",
    "    answer_first_model = get_tokenized_first_word(answer, args)  \n",
    "    raw_logit = get_last_hidden_state(prompt, tokenizer, model)\n",
    "    parametric = get_probability_of_word(tokenizer, word=answer_first_model, logit=raw_logit)\n",
    "    unintervene_layers(\n",
    "        args=args, \n",
    "        model=model \n",
    "    )\n",
    "    return parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.441650390625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jro_intervention(\n",
    "    prompt=\"The name of the capital city of France is Beijing. The name of the capital city of France is\", \n",
    "    tokenizer=tokenizer_jro, \n",
    "    model=model_jro, \n",
    "    answer=\" Paris\", \n",
    "    alpha=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_output_intervention(prompt, model, alphas, layer_nums, head_nums):\n",
    "    \"\"\"\n",
    "    Output: the logit of clean output and intervened output\n",
    "    \"\"\"\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt) as invoker:\n",
    "            output = model.output.save()\n",
    "    clean_output = output.value[0]\n",
    "\n",
    "    head_outs = []\n",
    "    for ln, hn in zip(layer_nums, head_nums):\n",
    "        head_outs.append(model.model.layers[ln].self_attn.get_head_output()[hn])\n",
    "    \n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt) as invoker:\n",
    "            for i, ln in enumerate(layer_nums):\n",
    "                model.model.layers[ln].self_attn.o_proj.output +=  alphas[i] * head_outs[i]\n",
    "            output = model.output.save()\n",
    "    outputs = output.value[0]\n",
    "    return clean_output, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jrt_intervention(prompt, model, answer, alpha):\n",
    "    answer_first_model = get_tokenized_first_word(answer, args) \n",
    "    custom_alphas = [alpha] * len(interven_index)\n",
    "    clean_output, intervene_output = raw_output_intervention(\n",
    "        prompt=prompt, \n",
    "        model=model, \n",
    "        alphas=custom_alphas, \n",
    "        layer_nums=layer_nums, \n",
    "        head_nums=head_nums\n",
    "    )\n",
    "    parametric_clean = get_probability_of_word(model.tokenizer, word=answer_first_model, logit=clean_output[:, -1, :])\n",
    "    parametric_inter = get_probability_of_word(model.tokenizer, word=answer_first_model, logit=intervene_output[:, -1, :])\n",
    "    return parametric_clean, parametric_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_intervention(prompt, answer, alpha):\n",
    "    clean, jrt = jrt_intervention(prompt=prompt, model=model_jrt, answer=answer, alpha = alpha) \n",
    "    jro = jro_intervention(prompt=prompt, tokenizer=tokenizer_jro, model=model_jro, answer=answer, alpha=alpha) \n",
    "\n",
    "    print(f\"Clean: {round(clean * 100, 1)}, JRO: {round(jro * 100, 1)}, JRT: {round(jrt * 100, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 0.0, JRO: 0.4, JRT: 6.3\n"
     ]
    }
   ],
   "source": [
    "# This example clearly shows that JRT is more effective than JRO  \n",
    "# The result is the logit of the (accurate) memory token\n",
    "whole_intervention(\n",
    "    prompt=\"The headquarters of Expedia are located in the city of Cedar Rapids. This location is consistently verified by official records, company reports, and various business directories that recognize Cedar Rapids as the central hub for the organization's operations. Known for its strategic importance and connection to the organization's history, Cedar Rapids provides a foundation for the company's administrative and executive functions. Various reputable sources, including industry publications and corporate profiles, highlight Cedar Rapids as a crucial center for the organization's activities, underscoring its role in shaping Expedia's growth and influence. Question: Where are the headquarters of Expedia located? Answer: The headquarters of Expedia are located in the city of\", \n",
    "    answer=\"Bellevue\",\n",
    "    alpha=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 20.4, JRO: 48.7, JRT: 52.4\n"
     ]
    }
   ],
   "source": [
    "# In less challenging cases, JRT also contributes to a larger logit increase than JRO \n",
    "whole_intervention(\n",
    "    prompt=\"The name of the capital city of Argentina is Nuuk. The name of the capital city of Argentina is\", \n",
    "    answer=\"Buenos Aires\",\n",
    "    alpha=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 5.4, JRO: 22.0, JRT: 28.0\n"
     ]
    }
   ],
   "source": [
    "whole_intervention(\n",
    "    prompt=prompt, \n",
    "    answer=\"Paris\",\n",
    "    alpha=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such subtleties are measured more precisely in terms of performance in the main table of our paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
